# (c) 2015-2017 Acellera Ltd http://www.acellera.com
# All Rights Reserved
# Distributed under HTMD Software License Agreement
# No redistribution in whole or part
#
import numpy as np
from scipy import stats
import random
from copy import deepcopy
from scipy.spatial import distance
import logging
import pickle
logger = logging.getLogger(__name__)


class MetricData:
    """ Class used for storing projected trajectories, their clustering and state assignments. Objects of this class
    are constructed by the `project` methods of the other projection classes. Only construct this class if you want to
    load saved data.

    Attributes
    ----------
    dat : numpy.ndarray
        The projected metrics
    ref : numpy.ndarray
        Reference indices to the simulations and frames that generated the metrics
    simlist : numpy.ndarray of :class:`Sim <htmd.simlist.Sim>` objects
        A simulation list generated by the :func:`simlist <htmd.simlist.simlist>` function
    fstep : float
        Size of simulation step in ns
    map : numpy.ndarray
        Contains the mapping from columns in `dat` to atom indices
    parent : :class:`MetricData` object
        The MetricData object that was used to generate this object
    St : numpy.ndarray
        Assignment of simulation frames to clusters
    K : int
        Number of clusters
    N : numpy.ndarray
        Populations of clusters
    Centers : numpy.ndarray
        Centers of clusters
    """
    
    def __init__(self, dat=None, ref=None, map=None, simlist=None, fstep=0, parent=None, file=None):
        self.dat = dat
        self.ref = ref
        self.simlist = simlist
        self.fstep = fstep
        self.map = map

        self.parent = parent

        self.St = None
        self.K = None
        self.N = None
        self.Centers = None

        if file is not None:
            self.load(file)

        self._dataid = random.random()
        self._clusterid = None
        return

    @property
    def trajLengths(self):
        """ Get the lengths of all trajectories

        Returns
        -------
        lens : list
            The lengths of all trajectories in the object

        Examples
        --------
        >>> data.trajLengths
        """
        return np.array([np.size(x, 0) for x in self.dat])

    @property
    def numFrames(self):
        """ Get the total number of frames in all trajectories

        Returns
        -------
        nframes : int
            Total number of frames in all trajectories

        Examples
        --------
        >>> data.numFrames
        """
        return sum(self.trajLengths)

    @property
    def numTrajectories(self):
        """ The number of trajectories

        Examples
        --------
        >>> data.numTrajectories
        """
        return len(self.dat)

    @property
    def numDimensions(self):
        """ The number of dimensions

        Examples
        --------
        >>> data.numDimensions
        """
        return self.dat[0].shape[1]

    @property
    def aggregateTime(self):
        """ The total aggregate simulation time

        Examples
        --------
        >>> data.aggTime
        """
        if self.fstep > 0:
            return self.numFrames * self.fstep

    def cluster(self, clusterobj, mergesmall=None, batchsize=False):
        """ Cluster the metrics

        Parameters
        ----------
        clusterobj : :class:`ClusterMixin <sklearn.cluster.ClusterMixin>` object
            The object of a clustering class from sklearn or with the same interface
        mergesmall : int
            Clusters containing less than `mergesmall` conformations will be joined into their closest well-populated
            neighbour.
        batchsize : int
            Batch sizes bigger than 0 will enable batching.

        Examples
        --------
        >>> from sklearn.cluster import MiniBatchKMeans
        >>> data = MetricDistance.project(sims, 'protein and name CA', 'resname MOL')
        >>> data.cluster(MiniBatchKMeans(n_clusters=1000), mergesmall=5)
        """
        #cluster_obj = coor.cluster_kmeans(self.dat, k=20, stride=1)
        if batchsize > 0:
            lengths = self.trajLengths
            currsum = 0
            starts = [0]
            for i, l in enumerate(lengths):
                currsum += l
                if currsum > batchsize:
                    starts.append(i+1)
                    currsum = 0
            starts.append(len(self.dat))
            for i in range(len(starts) - 1):
                clusterobj.partial_fit(np.concatenate(self.dat[starts[i]:starts[i+1]]))
            labels = []
            for i in range(len(self.dat)):
                labels.append(clusterobj.predict(self.dat[i].astype(np.float32)))
            # This is retarded
            labels = np.concatenate(labels)
        else:
            datconcat = np.concatenate(self.dat)
            if np.ndim(datconcat) == 1:
                datconcat = np.transpose(np.atleast_2d(datconcat))
            import warnings  # Following 3 lines are BS because sklearn refuse to make releases more often than 1 per year...
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                clusterobj.fit(datconcat)
            labels = clusterobj.labels_

        uqclu = np.unique(labels)
        self.Centers = clusterobj.cluster_centers_[uqclu, :]
        self.K = len(uqclu)
        # ---- Fixing missing clusters crap...
        map = np.zeros(np.max(labels)+1, dtype=int) * -1
        map[uqclu] = range(self.K)
        labels = map[labels]
        # ------------------------------------
        self.St = self.deconcatenate(labels)
        self.N = np.bincount(labels)

        if mergesmall is not None:
            oldK = self.K
            self.K, self.St, self.Centers, self.N, xxx = _mergeSmallClusters(mergesmall, datconcat, labels, self.Centers, self.N)
            logger.info('Mergesmall removed {} clusters. Original ncluster {}, new ncluster {}.'.format(oldK-self.K, oldK, self.K))
            self.St = self.deconcatenate(self.St)

        self._dataid = random.random()
        self._clusterid = self._dataid
    
    def combine(self, otherdata):
        """ Combines two different metrics into one by concatenating them.

        Parameters
        ----------
        otherdata : :class:`MetricData` object
            Concatenates the metrics of otherdata to the current objects metrics

        Examples
        --------
        >>> dataRMSD = MetricRmsd.project(sims)
        >>> dataDist = MetricSelfDistance.project(sims, 'protein and name CA')
        >>> dataRMSD.combine(dataDist)
        """
        if not np.array_equal(self.trajLengths, otherdata.trajLengths):
            raise NameError('Trying to combine MetricData objects with different number/lengths of trajectories. Check the trajLengths property.')
        for i in range(self.numTrajectories):
            if self.simlist[i].simid != otherdata.simlist[i].simid:
                raise NameError('Simulation ids do not match. Cannot combine. Please generate both data from the same simlist')
        for i in range(self.numTrajectories):
            self.dat[i] = np.concatenate((self.dat[i], otherdata.dat[i]), axis=1)
        self.map = self.map.append(otherdata.map, ignore_index=True)
        self._dataid = random.random()

    def dropDimensions(self, drop=None, keep=None):
        """ Drop some dimensions of the data given their indexes

        Parameters
        ----------
        drop : list
            A list of integer indexes of the dimensions to drop
        keep : list
            A list of integer indexes of the dimensions to keep

        Examples
        --------
        >>> data.dropDimensions([1, 24, 3])
        >>> data.dropDimensions(keep=[2, 10])
        """
        if drop is not None and not isinstance(drop, np.ndarray):
            drop = np.array(drop)
        if keep is not None and not isinstance(keep, np.ndarray):
            keep = np.array(keep)
        if drop is not None and keep is not None:
            raise AttributeError('drop and keep arguments for dropDimensions are mutually exclusive. Pass only one.')
        if keep is not None:
            keepidx = keep
            dropidx = np.arange(self.numDimensions)
            dropidx = np.setdiff1d(dropidx, keepidx)
        else:
            dropidx = drop
            keepidx = np.arange(self.numDimensions)
            keepidx = np.setdiff1d(keepidx, dropidx)

        for i, d in enumerate(self.dat):
            self.dat[i] = self.dat[i][:, keepidx]
        self.map = self.map.drop(self.map.index[dropidx])
        self.map = self.map.reset_index(drop=True)

    def dropTraj(self, limits=None, multiple=None, partial=None, idx=None, keepsims=None):
        """ Drops trajectories based on their lengths

        By default, drops all trajectories which are not of statistical mode (most common) length.

        Parameters
        ----------
        limits : list, optional
            Lower and upper limits of trajectory lengths we want to keep. e.g. [100, 500]
        multiple : list, optional
            Drops trajectories whose length is not a multiple of lengths in the list. e.g. [50, 80]
        partial : bool
            Not implemented yet
        idx : list, optional
            A list of trajectory indexes to drop
        keepsims : list of :class:`Sim <htmd.simlist.Sim>` objects
            A list of sims which we want to keep

        Examples
        --------
        >>> data = MetricSelfDistance.project(sims, 'protein and name CA')
        >>> data.dropTraj()
        >>> data.dropTraj(multiple=[100])
        """
        trajLengths = self.trajLengths
        orgNum = len(self.dat)

        if limits is not None:
            drop = (trajLengths < limits[0]) | (trajLengths > limits[1])
        elif multiple is not None:
            if partial is not None:
                raise NameError('TODO')
                pass
            idx = range(orgNum)
            for i in range(len(multiple)):
                idxNew = np.where(np.mod(trajLengths, multiple[i]) != 0)
                idx = np.intersect1d(idxNew, idx)
            drop = np.zeros(len(self.dat), dtype=bool)
            drop[idx] = True
        elif idx is not None:
            drop = np.zeros(len(self.dat), dtype=bool)
            drop[idx] = True
        elif keepsims is not None:
            drop = np.ones(len(self.dat), dtype=bool)
            for i, s in enumerate(self.simlist):
                for k in keepsims:
                    if s == k:
                        drop[i] = False
        else:
            drop = trajLengths != np.array(stats.mode(trajLengths).mode)

        keep = np.invert(drop)
        dropIdx = np.where(drop)[0]

        self.dat = self.dat[keep]
        self.ref = self.ref[keep]
        self.simlist = self.simlist[keep]
        self._dataid = random.random()
        if self.parent:
            self.parent.dat = self.parent.dat[keep]
            self.parent.ref = self.parent.ref[keep]
            self.parent.simlist = self.parent.simlist[keep]
            self.parent._dataid = random.random()

        logger.info('Dropped ' + str(np.sum(drop)) + ' trajectories from ' + str(orgNum) + ' resulting in ' + str(len(self.dat)))
        return dropIdx

    def bootstrap(self, ratio, replacement=False):
        """ Randomly sample a set of trajectories

        Parameters
        ----------
        ratio : float
            What ratio of trajectories to keep. e.g. 0.8
        replacement : bool
            If we should sample with replacement

        Returns
        -------
        bootdata : :class:`MetricData` object
            A new :class:`MetricData` object containing only the sampled trajectories

        Examples
        --------
        >>> data = MetricSelfDistance.project(sims, 'protein and name CA')
        >>> databoot = data.bootstrap(0.8)
        """
        numtraj = self.numTrajectories
        numtokeep = int(np.floor(numtraj * ratio))
        if replacement:
            rndtraj = np.random.randint(numtraj, size=numtokeep)
        else:
            rndtraj = np.random.permutation(numtraj)[0:numtokeep]
        rndtraj = sorted(rndtraj)  # Important to keep the sorting! i.e. for data.dropTraj(keepsims=sims)

        pp = None
        if self.parent is not None:
            pp = self.parent.copy()
            pp.dat = self.parent.dat[rndtraj]
            pp.ref = self.parent.ref[rndtraj]
            pp.simlist = self.parent.simlist[rndtraj]
            pp._dataid = random.random()
        bootdata = MetricData(dat=self.dat[rndtraj], ref=self.ref[rndtraj], map=self.map, simlist=self.simlist[rndtraj], parent=pp, fstep=self.fstep)
        return bootdata
    
    def plotTrajSizes(self):
        """ Plot the lengths of all trajectories in a sorted bar plot

        Examples
        --------
        >>> data = MetricSelfDistance.project(sims, 'protein and name CA')
        >>> data.plotTrajSizes()
        """
        trajLengths = self.trajLengths * self.fstep
        import matplotlib.pyplot as plt
        plt.ion()
        _ = plt.hist(trajLengths)
        #plt.bar(range(len(trajLengths)), np.sort(trajLengths), color='b', edgecolor='b')
        plt.ylabel('Num trajectories')
        plt.xlabel('Length of trajectories (in ns)')
        plt.show()
        return
    
    def splitCols(self):
        raise NameError('Not implemented yet')

    def deconcatenate(self, array):
        sizes = [np.size(x, 0) for x in self.dat]
        indeces = np.cumsum(sizes)
        if np.ndim(array) == 1:
            return np.array(np.split(array, indeces[:-1]))
        else:
            return np.array(np.vsplit(array, indeces[:-1]))

    def abs2rel(self, absFrames):
        """ Convert absolute frame indexes into trajectory index-frame pairs

        Useful when doing calculations on a concatenated data array of all trajectories. When you find a frame of
        interest you can `deconcatenate` the frame index to the corresponding trajectory index-frame pair.

        Parameters
        ----------
        absFrames : list of int
            A list of absolute index frames

        Returns
        -------
        pairs : np.ndarray
            A array where each row is a trajectory index-frame pair

        Examples
        --------
        >>> relidx = data.abs2rel(536)
        """
        if not hasattr(absFrames, "__len__"):
            absFrames = [absFrames]
        endFrames = np.append(0, np.cumsum(self.trajLengths))

        relframe = np.zeros((len(absFrames), 2), dtype=int)
        for i in range(len(absFrames)):
            trajIdx = np.where(absFrames[i] < endFrames)[0][0] - 1
            trajFr = absFrames[i] - endFrames[trajIdx]
            relframe[i, :] = [trajIdx, trajFr]
        return relframe

    def rel2sim(self, relFrames, simlist=None):
        """ Converts trajectory index-frame pairs into Sim-frame pairs

        Parameters
        ----------
        relFrames : 2D np.ndarray
            An array containing in each row trajectory index and frame pairs
        simlist : numpy.ndarray of :class:`Sim <htmd.simlist.Sim>` objects
            Optionally pass a different (but matching, i.e. filtered) simlist for creating the Frames.

        Returns
        -------
        frames : np.ndarray
            An array of :class:`Frame <htmd.simlist.Frame>` objects containing the simulation object, the trajectory
            piece ID and the frame index.

        Examples
        --------
        >>> simframes = data.rel2sim([100, 56])  # 100th simulation frame 56
        """
        from htmd.simlist import Frame
        if simlist is None:
            simlist = self.simlist
        else:
            if len(simlist) != len(self.simlist):
                raise AttributeError('Provided simlist has different number of trajectories than the one used by this object.')

        relFrames = np.array(relFrames)
        if relFrames.ndim == 1:
            relFrames = relFrames[np.newaxis, :]

        sims = []
        frames = []
        for i in range(np.size(relFrames, 0)):
            trajID = relFrames[i, 0]
            trajFrame = relFrames[i, 1]
            sims.append(simlist[trajID])
            frames.append(Frame(simlist[trajID], self.ref[trajID][trajFrame, 0], self.ref[trajID][trajFrame, 1]))
        return np.array(frames)

    def abs2sim(self, absFrames):
        """ Converts absolute frame indexes into Sim-frame pairs

        Parameters
        ----------
        absFrames : list of int
            A list of absolute index frames

        Returns
        -------
        frames : np.ndarray
            An array of :class:`Frame <htmd.simlist.Frame>` objects containing the simulation object, the trajectory
            piece ID and the frame index.

        Examples
        --------
        >>> simframes = data.abs2sim(563)  # 563rd frame to simulation/frame pairs
        """
        return self.rel2sim(self.abs2rel(absFrames))

    def copy(self):
        """ Produces a deep copy of the object

        Returns
        -------
        data : :class:`MetricData` object
            A copy of the current object

        Examples
        --------
        >>> data = MetricSelfDistance.project(sims, 'protein and name CA')
        >>> data2 = data.copy()
        """
        return deepcopy(self)

    def save(self, filename):
        """ Save a :class:`MetricData` object to disk

        Parameters
        ----------
        filename : str
            Path of the file in which to save the object

        Examples
        --------
        >>> data = MetricSelfDistance.project(sims, 'protein and name CA')
        >>> data.save('./data.dat')
        """
        #np.save(filename, [self.__dict__[k] for k in self.__dict__])
        parentpointer = self.parent
        if self.parent is not None:
            self.parent = self.parent.__dict__

        f = open(filename, 'wb')
        pickle.dump(self.__dict__, f)
        f.close()

        if self.parent is not None:
            self.parent = parentpointer

    def load(self, filename):
        """ Load a :class:`MetricData` object from disk

        Parameters
        ----------
        filename : str
            Path to the saved MetricData object

        Examples
        --------
        >>> data = MetricData()
        >>> data.load('./data.dat')
        """
        #z = np.load(filename)
        #for i, k in enumerate(self.__dict__):
        #    self.__dict__[k] = z[i]
        f = open(filename, 'rb')
        z = pickle.load(f)
        f.close()
        for k in self.__dict__:
            try:
                self.__dict__[k] = z[k]
            except:
                logger.warning('Could not find class property {} in file {}'.format(k, filename))
        if self.parent is not None:
            parentdict = self.parent
            self.parent = MetricData()
            for k in self.parent.__dict__:
                try:
                    self.parent.__dict__[k] = parentdict[k]
                except:
                    logger.warning('Could not find class property {} in file {}'.format(k, filename))

    def _defaultLags(self):
        modelen = stats.mode(self.trajLengths).mode - 1  # -1 to avoid warnings in timescales calc
        if modelen > 20:
            lags = np.append(1, np.round(np.linspace(10, modelen, 25)))
        else:
            lags = np.append(1, np.round(np.linspace(2, modelen, 25)))
        return lags.astype(int)

    def _contourPlot(self, x, y, z=None, resolution=100, levels=100, logplot=False, cmap=None, title=None, xlabel=None, ylabel=None):
        """ Plots a contour plot.

        If only x, y are given it will calculate a histogram for the contours. If z is given it will use that instead.
        """
        from matplotlib import pylab as plt
        from matplotlib.mlab import griddata

        if cmap is None:
            cmap = plt.cm.Greys

        f = plt.figure()
        ax = f.gca()
        if z is None:
            # If no z is given calculate z as a histogram
            zi, xi, yi = np.histogram2d(x, y, bins=resolution)
            zi = zi.T
            if logplot:
                zi = -np.log(zi)
            xi = xi[:-1] + (xi[1] - xi[0]) / 2  # Convert edges to bin centers
            yi = yi[:-1] + (yi[1] - yi[0]) / 2  # Convert edges to bin centers
        else:
            # Else if z is given interpolate on a grid
            xi = np.linspace(np.min(x), np.max(x), resolution)
            yi = np.linspace(np.min(y), np.max(y), resolution)
            zi = griddata(x, y, z, xi, yi, interp='linear')
        cf = ax.contourf(xi, yi, zi, levels, cmap=cmap)
        _ = ax.axis('equal')
        if title is not None:
            ax.set_title(title)
        if xlabel is not None:
            ax.set_xlabel(xlabel)
        if ylabel is not None:
            ax.set_ylabel(ylabel)
        return f, ax, cf

    def _setColorbar(self, f, mappable, label=None, scientific=True):
        import matplotlib.ticker as ticker
        def fmt(x, pos):
            a, b = '{:.2e}'.format(x).split('e')
            b = int(b)
            return r'${} \times 10^{{{}}}$'.format(a, b)
        if scientific:
            f.colorbar(mappable, format=ticker.FuncFormatter(fmt), label=label)
        else:
            f.colorbar(mappable, label=label)


    def plotCounts(self, dimX, dimY, resolution=100, logplot=False):
        """ Plots a histogram of counts on any two given dimensions.

        Parameters
        ----------
        dimX : int
            Index of projected dimension to use for the X axis.
        dimY : int
            Index of projected dimension to use for the Y axis.
        resolution : int
            Resolution of bincount grid.
        logplot : bool
            Set True to plot the logarithm of counts.
        """
        from matplotlib import pylab as plt
        dc = np.concatenate(self.dat)
        if self.map is not None:
            xlabel = self.map.description[dimX]
        else:
            xlabel = 'Dimension {}'.format(dimX)
        if self.map is not None:
            ylabel = self.map.description[dimY]
        else:
            ylabel = 'Dimension {}'.format(dimY)
        title = 'Counts histogram'

        f, ax, cf = self._contourPlot(dc[:, dimX], dc[:, dimY], resolution=resolution, xlabel=xlabel, ylabel=ylabel, title=title, logplot=logplot)
        self._setColorbar(f, cf, 'Counts')
        # f.show() Raises warnings in notebooks
        plt.show()

    def plotClusters(self, dimX, dimY, resolution=100, s=4, c=None, cmap=None, logplot=False):
        """ Plot a scatter-plot of the locations of the clusters on top of the count histogram.

        Parameters
        ----------
        dimX : int
            Index of projected dimension to use for the X axis.
        dimY : int
            Index of projected dimension to use for the Y axis.
        resolution : int
            Resolution of bincount grid.
        s : float
            Marker size for clusters.
        c : list
            Colors or indexes for each cluster.
        cmap : matplotlib.colors.Colormap
            Matplotlib colormap for the scatter plot.
        logplot : bool
            Set True to plot the logarithm of counts.
        """
        if self.Centers is None:
            raise RuntimeError('Data has not been clustered yet. Cannot plot clusters.')
        from matplotlib import pylab as plt
        if cmap is None:
            cmap = plt.cm.jet
        if self.map is not None:
            xlabel = self.map.description[dimX]
        else:
            xlabel = 'Dimension {}'.format(dimX)
        if self.map is not None:
            ylabel = self.map.description[dimY]
        else:
            ylabel = 'Dimension {}'.format(dimY)
        title = 'Clusters plotted onto counts histogram'
        dc = np.concatenate(self.dat)
        cent = self.Centers
        f, ax, cf = self._contourPlot(dc[:, dimX], dc[:, dimY], resolution=resolution, xlabel=xlabel, ylabel=ylabel, title=title, logplot=logplot)
        y = ax.scatter(cent[:, dimX], cent[:, dimY], s=s, c=c, cmap=cmap, linewidths=0, marker='o')
        if c is not None:
            self._setColorbar(f, y, 'Cluster groups')
        # f.show() Raises warnings in notebooks
        plt.show()

    def __repr__(self):
        return '<{}.{} object at {}>\n'.format(self.__class__.__module__, self.__class__.__name__, hex(id(self))) \
               + self.__str__()

    def __str__(self):
        def formatstr(name, field):
            if isinstance(field, np.ndarray) or isinstance(field, list):
                rep = '{} shape: {}'.format(name, np.shape(field))
            else:
                rep = '{}: {}'.format(name, field)
            return rep

        rep = 'MetricData object with {} trajectories of {}ns aggregate simulation time'.format(self.numTrajectories, self.aggregateTime)
        for j in sorted(self.__dict__.keys()):
            if j[0] == '_':
                continue
            if j == 'parent':
                rep += '\nparent: {} at {}'.format(type(self.parent), hex(id(self.parent)))
            elif j == 'map':
                rep += '\nmap: {} at {}'.format(type(self.map), hex(id(self.map)))
            else:
                rep += '\n'
                rep += formatstr(j, self.__dict__[j])

        return rep


def _mergeSmallClusters(mergesmall, data, stconcat, centers, N, metric=None):
    if data.dtype == 'bool':
        metric = 'hamming'
    else:
        metric = 'euclidean'
    badclusters = N < mergesmall
    goodclusters = np.invert(badclusters)
    N[badclusters] = 0
    badcluidx = np.where(badclusters)[0]
    goodcluidx = np.where(goodclusters)[0]

    if len(badcluidx) == 0:
        return len(N), stconcat, centers, N, badclusters

    # Keep only good centers
    centers = centers[goodclusters, :]

    # Creating a mapping for new cluster numbers as we will have to remove some
    newidx = np.zeros(len(N), dtype=int)
    newidx[goodcluidx] = range(len(goodcluidx))

    # Find all frames which belong to bad clusters
    frames = _ismember(stconcat, badcluidx)
    badframeidx = np.where(frames >= 0)[0]

    # Calculate distance of all frames belonging to bad clusters to the good cluster centers
    dists = distance.cdist(np.atleast_2d(data[badframeidx, :]), np.atleast_2d(centers), metric)
    minidx = np.argmin(dists, axis=1)  # Find closest center. Indexes are relative to goodidx
    newclu = goodcluidx[minidx]  # Back to absolute cluster indexes

    # Reassign bad frames to good clusters
    stconcat[badframeidx] = newclu    # Assign them to new clusters
    stconcat = newidx[stconcat]  # Convert all cluster indexes to the new indexes

    N = np.bincount(stconcat)
    K = len(N)
    return K, stconcat, centers, N, badclusters


def _ismember(a, b):
    bind = {}
    for i, elt in enumerate(list(set(b))):
        bind[elt] = i
    return np.array([bind.get(itm, -1) for itm in a])  # None can be replaced by any other "not in b" value


if __name__ == '__main__':
    from htmd import *
    from htmd.util import tempname
    from htmd.home import home
    from os.path import join

    testfolder = home(dataDir='adaptive')

    sims = simlist(glob(join(testfolder, 'data', '*', '')), glob(join(testfolder, 'input', '*', 'structure.pdb')))
    fsims = simfilter(sims, tempname(), 'not water')
    metr = Metric(fsims)
    metr.set(MetricDistance('protein and resid 10 and name CA', 'resname BEN and noh', metric='contacts',
                            groupsel1='residue', threshold=4))
    data1 = metr.project()
    metr.set(MetricDihedral())
    data2 = metr.project()

    # Testing combining of metrics
    data1.combine(data2)

    # Testing dimensions
    assert np.array_equal(data1.map.shape, (897, 3)), 'combine not working correct'
    assert np.array_equal(data1.dat[0].shape, (6, 897)), 'combine not working correct'
    assert np.array_equal(np.where(data1.map.type == 'contact')[0], [0, 1, 2, 3, 4, 5, 6, 7, 8]), 'combine not working correct'

    # Testing dimension dropping / keeping
    datatmp = data1.copy()
    data1.dropDimensions(range(9))
    assert np.array_equal(data1.map.shape, (888, 3)), 'dropDimensions not working correct'
    assert np.array_equal(data1.dat[0].shape, (6, 888)), 'dropDimensions not working correct'
    assert len(np.where(data1.map.type == 'contact')[0]) == 0, 'dropDimensions not working correct'
    data1 = datatmp.copy()
    data1.dropDimensions(keep=range(9))
    assert np.array_equal(data1.map.shape, (9, 3)), 'dropDimensions not working correct'
    assert np.array_equal(data1.dat[0].shape, (6, 9)), 'dropDimensions not working correct'
    assert len(np.where(data1.map.type == 'dihedral')[0]) == 0, 'dropDimensions not working correct'



